[{"content":"Recently, I have been working on Red Hat Advanced Cluster Security (RHACS) to identify security vulnerabilities and implementing cluster hardening rules within OpenShift clusters. Inspired by my experiences, I decided to share my insights through this blog post. Join me as we delve into the world of RHACS and explore its functionality.\nIntroduction to RHACS Red Hat Advanced Cluster Security (RHACS) is the downstream project for the upstream Stackrox project. In other words, enhancements to the source code are initially made in Stackrox, and then they undergo testing and packaging to become part of RHACS. There are three primary functionalities for which I am using RHACS:\nVulnerability Management Workload Hardening Policies Integration with the Compliance Operator In this blog, we will focus on the first two functionalities, as the Compliance Operator is a comprehensive topic that deserves its own dedicated discussion\nVulnerability Management RHACS provides a robust mechanism to scan all the images that are deployed in your cluster. It checks all the images you use and breaks them down into layers. Then it looks inside each layer to find out which software packages are there. After that, it compares these packages with a special database that collects information about Common Vulnerabilities and Exposures (CVE). This database gathers data from various sources, including Open-Source Vulnerability (OSV), Debian Security Tracker, Red Hat OVAL, National Vulnerability Database (NVD), and more.\nVulnerability Data Process RHACS runs a process that involves several steps. First, it downloads the latest vulnerability data from the above mentioend sources. Next, it converts this data into a format that an in-house scanner can understand. Currently, we have two different in-house scanners: StackRox Scanner (Scanner v2) and Scanner V4. Once the data is processed, it is pushed to definitions.stackrox.io. This entire process occurs every 3 hours.\nWithin the cluster, there are three separate processes that play crucial roles:\nCentral: Central queries definitions.stackrox.io every 5 minutes to fetch the most up-to-date vulnerability data. If there are no updates, no new data is sent.\nScanner: The scanner queries Central approximately every 5 minutes for the latest vulnerability data. Again, if there are no updates, no new data is sent.\nCentral Reprocessing Pipeline: Central periodically runs a “reprocessing” pipeline. During this process - all policies are re-evaluated, images are re-scanned, the scanners download each layer of an image, extract necessary information, and store it. While the “analysis” part may not be redone (since images are immutable), the scanners still check for any new vulnerabilities that may affect the image. By default, this reprocessing occurs every 4 hours. FYI, You can adjust the frequency by configuring the ROX_REPROCESSING_INTERVAL environment variable in Central. Keep in mind that this operation is CPU-intensive, especially for Central.\nVulnerability Report RHACS provides a comprehensive vulnerability report. This report includes details about vulnerabilities within your OpenShift environment. For each vulnerability, it specifies the affected namespace, deployment, image, package, version, and remediation steps. Additionally, the report highlights the CVSS score, CVE number, and whether an update is available. RHACS helps you stay informed about vulnerabilties in your cluster. You can also watch images that might not be a part of your cluster with vulnerability management.\nWorkload Hardening Policies Policies are nothing but security best practices that should be applied consistently across all workloads within a cluster. While there may be exceptions or exclusions to specific policies, it remains crucial that resources adhere to these established security practices. There are three lifecycle phases in which policies are divided.\nBuild Policies: These policies report violations during the creation of container images. For instance, they target best practices for building containers, such as avoiding the use of images with Important severity CVEs or executing all commands in the container as the root user during image build. Deployment Policies: Violations are reported at the time of deployment to the OpenShift cluster. These policies consist of best practices, such as avoiding the storage of secrets in environment variables or ensuring that service account tokens are not mounted until the service account is actually being used. Runtime Policies: These policies monitor violations while workloads are active on the OpenShift cluster. Violations reported at runtime include actions like executing systemctl inside a container, running nmap within a workload, or executing iptables. There are in total 86 policies that are provided by default RHACS at the time of writing this blog. RHACS provides the ability to create custom policies as well.\nWithin the cluster, violations related to Deployment and Runtime policies are visible on the Violations dashboard in RHACS. In contrast, Build policies undergo checks during Continuous Integration pipelines. To perform CI checks, the roxctl command-line tool offers three specific commands for build-time analysis.\n# Scan the specified image, and return scan results $ roxctl image scan #Check images for build time policy violations, and report them $ roxctl image check # Check deployments for deploy time policy violations $ roxctl deployment check Example of a Gitlab pipelines to check image and deployment policies:\ncheck-image: stage: scan image: registry-redhat.io/advanced-cluster-security/rhacs-roxctl-rhel8:3.69.0 script: - \u0026#39;curl -k -H \u0026#34;Authorization: Bearer ${ROX_API_TOKEN}\u0026#34; \u0026#34;${ROX_CENTRAL_HOST}\u0026#34;:443/api/cli/download/roxctl-linux -o roxctl \u0026amp;\u0026amp; chmod +x ./roxctl\u0026#39; - ./roxctl --insecure-skip-tls-verify -e \u0026#34;${ROX_CENTRAL_HOST}:443\u0026#34; image check --image=\u0026lt;image-location\u0026gt; check-deployment: stage: scan image: registry-redhat.io/advanced-cluster-security/rhacs-roxctl-rhel8:3.69.0 script: - set -x - \u0026#39;curl -k -H \u0026#34;Authorization: Bearer ${ROX_API_TOKEN}\u0026#34; \u0026#34;${ROX_CENTRAL_HOST}\u0026#34;:443/api/cli/download/roxctl-linux -o roxctl \u0026amp;\u0026amp; chmod +x ./roxctl\u0026#39; - ./roxctl --insecure-skip-tls-verify -e \u0026#34;${ROX_CENTRAL_HOST}:443\u0026#34; deployment check --file=\u0026lt;deployment-location\u0026gt; Conclusion Vulnerability Management: CVE data is fetched from sources every 3 hours. Once fetched, information is stored at definitions.stackrox.io Central queries definitions.stackrox.io every 5 minutes to fetch vulnerabilities. Scanner\u0026rsquo;s job is to get the information from Central and compare it with the workload in your cluster. Hardening Policies: Policies are divided into three lifecycle phases: build, deploy, and runtime. Build policies are part of the shift left initiative. By catching issues early in the development process, build policies enhance overall security. ","permalink":"https://journalctl.org/post/rhacs/","summary":"Recently, I have been working on Red Hat Advanced Cluster Security (RHACS) to identify security vulnerabilities and implementing cluster hardening rules within OpenShift clusters. Inspired by my experiences, I decided to share my insights through this blog post. Join me as we delve into the world of RHACS and explore its functionality.\nIntroduction to RHACS Red Hat Advanced Cluster Security (RHACS) is the downstream project for the upstream Stackrox project. In other words, enhancements to the source code are initially made in Stackrox, and then they undergo testing and packaging to become part of RHACS.","title":"Unlocking Stackrox/RHACS (Part 1): A Comprehensive Guide to Container Security"},{"content":"Open Source Software (OSS) projects have been distributed in packages for decades. Using packages allows developers to focus on new feature implementation. Major software distributions, such as Fedora, Debian, etc, typically consist of thousands of packages. These packages depend on each other to perform tasks efficiently by avoiding code duplication. The inter-dependence amongst the packages creates a software supply chain.\nExamples of software security principles such as using software binaries that are signed by the software vendor, keeping binaries regularly updated, and constantly monitoring software behaviour are general to any software system. However, following these principles may not suffice for preventing a software supply chain from being infected with malicious code. Software supply chains are far from being simple. Recent supply chain attacks such as SolarWinds and Mimecast have raised serious security concerns. Despite the best effort of developers in addressing security concerns, any software project is prone to supply chain attacks. This is observed even in the context of major software organizations.\nSecuring your software supply chain goes beyond basic principals, it demands a proactive and adaptive appoach, especially when sophisticated threats are seen in the recent attacks. Renovate is becoming a crucial tool in this effort. Renovate, a powerful dependency management tool, steps in to enhance your defense strategy. Unlike traditional update approaches, Renovate actively monitors and patches vulnerabilities, making it a key player in safeguarding your software supply chain. In this blog, we’ll explore how integrating Renovate into GitLab CI pipelines can elevate your security measures, ensuring your project stays resilient against evolving cyber threats.\nIMPLEMENTATION APPROACH There are multiple ways to configure the GitLab Runner, which is responsible for running CI/CD pipelines on GitLab. In the context of this implementation, each CI/CD pipeline runs in a separate container within the OpenShift instance. To facilitate this, a Dockerfile is used to generate the container image that will be executed through OpenShift. In this implementation, a container image is generated for Renovate, and this container image is executed as a cron job resource within the OpenShift cluster.\nDockerfile Few key points to take note from the Dockerfile provided below are:\nWe need to install the go and java programming languages in the Dockerfile to be able to update the go.sum and pom.xml files. Renovate works by inspecting configuration files in the application projects to identify the dependencies and their versions. Since we install the go programming language from source, we verify its checksum to avoid any discrepancies and thereby refrain from installing malicious code We run the Renovate command as a non-root user. FROM node:latest USER root RUN mkdir -p /etc/rhsm/ca \u0026amp;\u0026amp; \\ rm -f /etc/rhsm-host RUN npm install --global renovate yarn \u0026amp;\u0026amp; \\ curl -sfL --retry 10 -o /tmp/golang.tar.gz https://go.dev/dl/go1.21.3.linux-amd64.tar.gz \u0026amp;\u0026amp; \\ echo \u0026#34;1241381b2843fae5a9707eec1f8fb2ef94d827990582c7c7c32f5bdfbfd420c8 /tmp/golang.tar.gz\u0026#34; | sha256sum -c \u0026amp;\u0026amp; \\ mkdir -p /usr/local/go \u0026amp;\u0026amp; \\ tar --strip-components=1 -zxf /tmp/golang.tar.gz --directory /usr/local/go \u0026amp;\u0026amp; \\ mkdir -p /go/{path,cache,xdg} \u0026amp;\u0026amp; \\ chmod -R 777 /go \u0026amp;\u0026amp; \\ chown -R default /opt/app-root/src RUN dnf install -y --disablerepo=* --enablerepo=ubi-9-baseos-rpms --enablerepo=ubi-9-appstream-rpms java-17-openjdk-devel USER 1001 ENV GOPATH /go/path ENV GOCACHE /go/cache ENV PATH $PATH:/usr/local/go/bin CMD [\u0026#34;renovate\u0026#34;] OpenShift Cron Job This cron job ensures that Renovate periodically checks and updates dependencies in specified GitLab repositories.\nspec: suspend: false schedule: \u0026#34;5 * * * *\u0026#34; startingDeadlineSeconds: 300 concurrencyPolicy: \u0026#34;Forbid\u0026#34; successfulJobsHistoryLimit: 3 failedJobsHistoryLimit: 1 jobTemplate: spec: activeDeadlineSeconds: 3600 template: metadata: labels: app: \u0026lt;app-name\u0026gt; component: \u0026#34;cron\u0026#34; spec: volumes: - name: data persistentVolumeClaim: claimName: renovate-pvc containers: - name: \u0026#34;renovate\u0026#34; image: \u0026lt;registry\u0026gt;/renovate env: - name: RENOVATE_PLATFORM value: \u0026#34;gitlab\u0026#34; - name: RENOVATE_ENDPOINT value: \u0026#34;https://\u0026lt;gitlab-repository\u0026gt;/api/v4/\u0026#34; - name: RENOVATE_TOKEN valueFrom: secretKeyRef: name: gitlab-secret key: gitlab_token - name: GITHUB_COM_TOKEN valueFrom: secretKeyRef: name: github-secret key: github_token - name: RENOVATE_REPOSITORIES value: \u0026lt;gitlab-repository\u0026gt;,\u0026lt;gitlab-repository\u0026gt;,\u0026lt;gitlab-repository\u0026gt; - name: RENOVATE_EXPOSE_ALL_ENV value: \u0026#34;true\u0026#34; - name: GIT_CONFIG_COUNT value: \u0026#34;1\u0026#34; - name: GIT_CONFIG_KEY_0 value: \u0026#34;safe.directory\u0026#34; - name: GIT_CONFIG_VALUE_0 value: \u0026#34;*\u0026#34; - name: LOG_LEVEL value: \u0026#34;INFO\u0026#34; restartPolicy: \u0026#34;Never\u0026#34; A secret needs to be created for storing the Gitlab token. Additionally, a volume with ReadWriteAccess within a configmap needs to be configured.\n","permalink":"https://journalctl.org/post/renovate/","summary":"Open Source Software (OSS) projects have been distributed in packages for decades. Using packages allows developers to focus on new feature implementation. Major software distributions, such as Fedora, Debian, etc, typically consist of thousands of packages. These packages depend on each other to perform tasks efficiently by avoiding code duplication. The inter-dependence amongst the packages creates a software supply chain.\nExamples of software security principles such as using software binaries that are signed by the software vendor, keeping binaries regularly updated, and constantly monitoring software behaviour are general to any software system.","title":"Enhancing Security: Using Renovate in Gitlab Pipelines for Automated Dependency Updates"},{"content":"Need for slimming down containers OpenShift, an enterprise-ready Kubernetes platform, offers a multitude of benefits. One such advantage is the Source-to-Image (S2I) build strategy, that simplifies the process of converting source code into deployable container images. This strategy enables developers to build container images without the need to define a container file explicitly. OpenShift clones the application\u0026rsquo;s source code into a builder image that utilizes builder scripts, ultimately generating a container image deployable within the cluster.\nEmploying the S2I strategy proves convenient, allowing software development teams to prioritize software development over the creation of container files. However, a notable drawback emerges as this approach tends to generate oversized images. These bulky images can rapidly consume storage capacity within private registries, resulting in prolonged push and pull times from the image registry. Beyond storage concerns, larger image sizes impact the build process, slowing down the overall build time. Moreover, heightened image size correlates with increased security risks, creating a larger attack surface for potential supply chain attacks due to the inclusion of numerous packages. Recognizing these challenges, the focus turns to exploring strategies aimed at slimming down containers.\nFour primary practices to overcome this issue Using small or minimal base images. Avoiding multiple RUN instructions. Excluding files and directories from the build context. Utilizing multi-stage container builds. Using small or minimal base image Using small or minimal base images for containers involves selecting lightweight starting points that contain only the essential packages required to run your application. Opting for such base images, like Alpine or distroless images, significantly reduces the container size. It is crucial to identify base images that tailor to specific application needs, ensuring they contain the necessary libraries and dependencies while excluding unnecessary components. Minimizing the attack surface by selecting base images with fewer pre-installed packages also enhances container security.\nAvoiding multiple RUN instructions Avoiding multiple RUN instructions in Dockerfiles involves consolidating commands into a single RUN instruction wherever possible. By chaining commands together, we can reduce the number of intermediate layers created during the image build process. For instance, rather than having separate RUN commands for installing dependencies, copying files, and performing cleanup, we can combine these steps within a single RUN instruction. Here\u0026rsquo;s an example:\n# Bad practice: Multiple RUN instructions FROM baseimage:latest RUN dnf update RUN dnf install -y package1 RUN dnf install -y package2 RUN dnf clean # Better practice: Consolidating commands in a single RUN instruction FROM baseimage:latest RUN dnf update \\ \u0026amp;\u0026amp; dnf install -y package1 package2 \\ \u0026amp;\u0026amp; dnf clean Excluding files and directories from the build context We can make use of the .containerignore or .dockerignore file in the build context directory to prevent unnecessary files and directories from being copied in to the image.\nAn example of .dockerignore file tailored for Ruby application:\n# Ignore version control files\r.git\r.gitignore\r# Ignore unnecessary editor/IDE files\r.vscode/\r.idea/\r# Ignore log and temp files\rlog/\rtmp/\r# Ignore dependency manager specific files\rGemfile.lock\r# Ignore development and test-specific files\rspec/\rtest/ Utilizing multi-stage container builds Multistage builds are implemented by defining multiple \u0026lsquo;stages\u0026rsquo; within a single Dockerfile. This strategy enables the separation of the build environment from the final runtime environment, leading to the creation of smaller and more secure container images. With multistage builds, each \u0026lsquo;stage\u0026rsquo; performs specific tasks, ensuring that the final container image only includes the necessary artifacts generated from the preceding build \u0026lsquo;stage.\u0026rsquo; This optimization results in a streamlined image tailored precisely to run the application, eliminating unnecessary packages or artifacts.\nAn example of the multi-stage build can be seen below:\n# Stage 1: Build stage FROM openjdk-17 AS builder WORKDIR /app COPY . . USER root RUN apt-get update \u0026amp;\u0026amp; apt-get install -y some-dependency RUN mvn -Dmaven.repo.local=/tmp/.m2/repository package # Stage 2: Runtime stage FROM openjdk17:alpine-jre WORKDIR /app COPY --from=builder --chown=someuser:somegroup /app/target/myapp.jar . USER someuser CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;myapp.jar\u0026#34;] The example above illustrates a two-stage process within a container file. The first stage handles all build operations, while the second stage exclusively copies the artifact generated in the first stage. This artifact is used for deploying the image to the cluster. Two key points emerge from this example:\nUsing different base image in the build and runtime stages. During the build stage, more packages may be necessary. For instance, in a Java application, we employ OpenJDK, the Java Development Kit (JDK), crucial for development and building. Conversely, the runtime stage utilizes the Java Runtime Environment (JRE), a subset of the JDK containing essential libraries to run the Java application. Hence, choosing base images tailored to specific needs is essential. In the build stage, we employ USER root to execute all build-related tasks. This approach often becomes necessary for tasks like installing dependencies and setting up the environment, requiring elevated privileges. However, once the necessary artifacts are acquired, transitioning to a non-root user USER someuser in subsequent stages enhances security. It minimizes potential vulnerabilities and restricts access privileges within the final runtime environment, improving overall security posture. Conclusion Lean container images are crucial for streamlined deployments, reducing storage overhead and minimizing resource consumption.\n","permalink":"https://journalctl.org/post/lean-container-images/","summary":"Need for slimming down containers OpenShift, an enterprise-ready Kubernetes platform, offers a multitude of benefits. One such advantage is the Source-to-Image (S2I) build strategy, that simplifies the process of converting source code into deployable container images. This strategy enables developers to build container images without the need to define a container file explicitly. OpenShift clones the application\u0026rsquo;s source code into a builder image that utilizes builder scripts, ultimately generating a container image deployable within the cluster.","title":"Slimming Down Containers: The Art of Minimizing Image Bloat"},{"content":"Containers are inherently ephemeral, making them more difficult to manage than traditional programmes operating on virtual or bare metal servers. Container monitoring, on the other hand, is a critical capability for applications based on current microservices architectures in order to achieve maximum performance.\nContainerized applications frequently necessitate monitoring. Performing health-checks is one technique to keep these containers up and running at all times. The usage of the curl command to verify if the application within the container is still up and running is one of the approaches I\u0026rsquo;ve come across for doing health-checks. The cronjob utility is used to perform curl commands on the container application at a periodic basis. If the container is not in a running state, a configuration management tool like Ansible can be used to start/restart it. Although this method does not need direct human intervention, it does not seem to have a mechanism for logging the event i.e. reason for downtime of the application/container. Therefore, it is not the best option for enhancing the container\u0026rsquo;s self-healing capabilities.\nInstead, we make use of the systemd utility of Linux which is ubiquitous across most of the Linux distributions. The systemd utility makes it easy to dynamically start/restart/stop configuration files for each deamon.\nIn this blog, we will:\nCreate an Ansible role. Define the required variables. Create the service file usign a Jinja2 template Manage unit files within the containers using a playbook With the aid of systemd, we\u0026rsquo;ll start an apache container and control its daemon. The Jinja2 templates are used to construct the unit file. We save the template files in the /usr/lib/systemd/system/ directory after they\u0026rsquo;ve been created. We can then use thesystemd ansible module to administer the deamon once the files have been stored in the correct location with the required permissions.\nCreate an Ansible role cd roles\rmkdir -p apache/{defaults,tasks,templates,handlers} Define the required variables ---\rapache_image: \u0026#34;httpd\u0026#34;\rapache_image_tag: \u0026#34;latest\u0026#34; Create the service file usign a Jinja2 template [Unit]\rDescription=run apache container\rAfter=network.target docker.service\rRequires=docker.service\r[Service]\rExecStartPre=-/usr/bin/docker stop apache\rExecStartPre=-/usr/bin/docker rm apache\rExecStartPre=/usr/bin/docker pull {{ apache_image }}:{{ apache_image_tag }}\rExecStart=/usr/bin/docker run \\\r--port=\u0026#39;8080:8080\u0026#39; \\\r--name=apache \\\r--volume={{ apache_data_dir }}:{{ apache_data_container_dir }}:ro \\\r--restart=always \\\r--log-driver=journald \\\r{{ apache_image }}:{{ apache_image_tag }}\rRestart=always\r[Install]\rWantedBy=multi-user.target Let\u0026rsquo;s break down the important parts:\nUnit Section: Description: Describes the purpose of the service. After: Defines the dependencies that must be started before this service. Requires: Specifies the units that this unit depends on. Service Section: ExecStartPre: Commands to be executed before starting the service. Here, it stops and removes any existing Docker container named \u0026lsquo;apache\u0026rsquo;. Then it pulls the specified Docker image (using variables apache_image and apache_image_tag provided during template rendering).\nExecStart: Command to start the Docker container with various options:\n\u0026ndash;port=\u0026lsquo;8080:8080\u0026rsquo;: Maps port 8080 from the container to the host.\n\u0026ndash;name=apache: Assigns the name \u0026lsquo;apache\u0026rsquo; to the container.\n\u0026ndash;volume={{ apache_data_dir }}:{{ apache_data_container_dir }}:ro: Mounts a directory from the host to a directory in the container in read-only mode.\n\u0026ndash;restart=always: Specifies that the container should always restart if it stops.\n\u0026ndash;log-driver=journald: Defines the logging driver for the container.\n{{ apache_image }}:{{ apache_image_tag }}: Specifies the Docker image and its tag to be used.\nRestart: Defines the restart policy for the service, set to \u0026lsquo;always\u0026rsquo;.\nInstall Section: WantedBy: Specifies the target that this service should be enabled for. Manage unit files within the containers using a playbook /roles/apache/handlers/main.yml\r---\r- name: restart container-apache\rsystemd:\rname: container-apache.service\rdaemon_reload: yes\rstate: restarted /roles/apache/tasks/main.yml\r---\r- name: set selinux boolean value\rseboolean:\rname: container_manage_cgroup\rstate: yes\rpersistent: yes\r- name: create the apache container unit file\rtemplate:\rsrc: container-apache.service.j2\rdest: /usr/lib/systemd/system/container-apache.service\rowner: root\rmode: 0660\rnotify: restart container-apache\r- name: link and enable container-apacahe\rsystemd:\rname: container-apache\renabled: yes\r- name: run apache container\rsystemd:\rname: container-apache\rstate: started ","permalink":"https://journalctl.org/post/systemd-containers/","summary":"Containers are inherently ephemeral, making them more difficult to manage than traditional programmes operating on virtual or bare metal servers. Container monitoring, on the other hand, is a critical capability for applications based on current microservices architectures in order to achieve maximum performance.\nContainerized applications frequently necessitate monitoring. Performing health-checks is one technique to keep these containers up and running at all times. The usage of the curl command to verify if the application within the container is still up and running is one of the approaches I\u0026rsquo;ve come across for doing health-checks.","title":"Managing Containers with Systemd"},{"content":"From May 10th to 12th, the Vancouver Convention Center came alive with the Open Source Summit North America, a three-day event that brought together open source software enthusiasts from around the world. With a focus on exploring the newest trends and embracing opportunities in the ever-changing field of open source technology, this premier conference drew over two thousand attendees. In this blog post, we\u0026rsquo;ll dive into my own presentation and its key points, share some highlights from the inspiring talks I attended, and take a closer look at the exciting conversations and new discoveries that unfolded at the Red Hat booth.\nDemystifying Unreproducible Builds: What, Why, and How? Soon after the keynote, the conference dispersed into multiple tracks. I presented in the Supply Chain Security track, where I discussed the Reproducible Builds initiative. My presentation covered several key topics:\nExploring software supply chains and the challenges of ensuring supply chain security, despite their benefits. Understanding reproducible builds and how they mitigate the risk of software supply chain attacks on packages within a Linux distribution. Identifying the software that should be prioritized first for reproducibility within a Linux distribution. Examining the time it takes for developers to address unreproducible builds and identifying potential causes. Investigating the influence of external ecosystem factors on reproducibility in package builds. By addressing these topics, I aimed to shed light on the importance of reproducible builds for enhancing supply chain security and ensuring the integrity of software packages within the Linux ecosystem.\nDuring my talk, I had the pleasure of meeting Mattia Rizzolo, a dedicated core maintainer of the Reproducible Builds project who had traveled all the way from Italy. It was fascinating to connect with someone so deeply involved in the subject matter. We discussed my research paper, which explored reproducible builds and was recently submitted to the Journal of Empirical Software Engineering. Mattia expressed genuine interest in reviewing it and even offered his assistance in addressing any potential rebuttals that may arise during the review process.\nMoreover, I was thrilled when Mattia appreciated the taxonomy of issues we had developed in the paper, which aimed to identify the causes of unreproducible builds. He went a step further and suggested that I open a pull request on the Reproducible Builds website to showcase this taxonomy. It was truly gratifying to receive recognition from a respected member of the project\u0026rsquo;s core team.\nAs our conversation continued, I couldn\u0026rsquo;t resist asking Mattia about his thoughts on the future of the Reproducible Builds project. He acknowledged that at present, many projects seem more inclined towards adopting trendy technologies and focusing on new developments, such as Software Bill of Materials (SBOM) files to secure the software supply chain. However, he raised a critical question: Can we truly guarantee that the SBOM file generated at the project level will remain unchanged for the customers and end users who consume it?\nEngaging sessions that caught my attention Culture Change: Building an OSPO in a Proprietary Software Company - Ruth Suehle, SAS In her talk, Ruth highlights that organizations in the software industry widely use open source software but questions their active contribution to the community. She cautions against the practice of solely forking and internally maintaining open source projects, which harms the community.\nFurthermore, Ruth advocates for establishing an Open Source Program Office (OSPO) team in proprietary software companies to address how they can contribute back. This team aligns values and actions with open source principles, sets guidelines, ensures compliance, and fosters collaboration. Lastly, Ruth emphasizes that embracing open source values leads to meaningful contributions and innovation, despite the time required for cultural change.\nThings I wish I had learned earlier about containers - Alex Juarez, Red Hat During the conference, Alex delivered an informative session that delved into the fundamental aspects of containers, making it an ideal starting point for beginners in the field. He covered several essential topics, including container runtimes, namespaces, cgroups, and binding volumes in containers. Additionally, he discussed the process of inspecting container images to identify exposed ports. If you\u0026rsquo;re interested in exploring the details further, you can find Alex\u0026rsquo;s slides for the session here.\nImplementing the OpenSSF Best Practices Badges \u0026amp; Scorecards Into Your Project - CRob, Intel \u0026amp; David A. Wheeler, The Linux Foundation Projects like Scorecard, operating under the OpenSSF umbrella, offer a valuable means to assess whether a software repository on GitHub adheres to important rules that mitigate supply chain attacks. These checks include the presence of a license file, the implementation of a CI pipeline for the project, and the utilization of a dependency manager such as Dependabot to analyze any vulnerabilities that may arise in current dependencies.\nAfter the talk, I had a brief discussion with David Wheeler regarding Scorecard\u0026rsquo;s limitations in evaluating proprietary software. Some of the checks were found to be incompatible with local systems, unlike the deployed version that performs checks on multiple repositories hosted on GitHub.\nMeeting David Wheeler in person was a source of great excitement for me. His blog post in 2021 about reproducible builds served as a motivating factor for my research. In his blog, he highlighted the importance of prioritizing certain software for reproducibility over others. This passage stimulated my interest in exploring the types of software that software developers should focus on in terms of reproducibility.\nInteresting conversations at the Red Hat Booth During my time at the Red Hat booth, I had the opportunity to engage in thought-provoking discussions with industry professionals.\nOne such encounter was with Jerry Cooperstein, an ex-Red Hatter who played a significant role in designing learning content for Red Hat\u0026rsquo;s courses and certifications. Currently, he leads efforts in designing courses and certifications for the Linux Foundation. Here are a few intriguing topics we covered:\nJerry expressed his skepticism towards multiple-choice question (MCQ) based certifications that lack practical application and fail to assess the practical aspects of technology. He emphasized the importance of incorporating practical evaluations into certification programs. We delved into a discussion about the effectiveness of different learning mediums. Jerry firmly believed that books and written content leave a more profound impact on learners compared to video courses. According to him, entertainment should be separate from the learning process. On a lighter note, Jerry shared an interesting anecdote about the placement of the control key on keyboards. He mentioned that, in the past, the control key was located where the Caps Lock key is positioned today. However, due to the strain it caused on the pinky finger, the key\u0026rsquo;s placement was eventually changed. During my conversation with the security software engineer at the Tenable booth, we delved deeper into the realm of container security. Our discussion covered various important topics, including the commonly performed security checks and best practices in the industry.\nWe explored the significance of following CIS benchmark guidelines, utilizing tools like Trivy or Clair to detect vulnerabilities, using muti-build container files, sanboxing containers (gVisor), and ensuring the use of security contexts for pods. These practices form the foundation for maintaining a secure container ecosystem.\nIntriguingly, our conversation expanded to address what differentiates companies like Tenable in this highly competitive industry. While acknowledging the similarity in security practices, the engineer highlighted a few additional insights that set companies apart. Some of these included:\nZero-day vulnerability: We discussed the importance of actively monitoring and addressing zero-day vulnerabilities, which are unknown and unpatched vulnerabilities that can be exploited by attackers. Lateral movement: This topic focused on understanding and mitigating the techniques used by attackers to move laterally within a network or system after gaining initial access. By identifying and preventing lateral movement, organizations can limit the impact of potential security breaches. Active checking for vulnerabilities: We explored the proactive approach of actively scanning and checking for vulnerabilities in containerized applications. By actively assessing and addressing vulnerabilities, organizations can reduce the risk of exploitation. Exclusion warriors: This term refers to individuals or teams responsible for managing exclusion lists or rules in security systems. Their role is to specify what should be excluded from security scans or monitoring, ensuring efficient and accurate security operations. I had seen the dockers adoption of wasm in Kubecon last year, but did pay much attention to the same. During a conversation at the Fermyon both, I was introduced to webassembly. I discovered for myself that WebAssembly (often abbreviated as Wasm) is a binary instruction format and virtual machine that enables the execution of high-performance, low-level code on the web. It is a portable, safe, and efficient technology designed to run in web browsers alongside JavaScript and other web languages.\nWebAssembly allows developers to compile code from various programming languages, such as C, C++, Rust, and more, into a compact binary format that can be executed by the web browser. This format is designed to be fast to load and execute, making it suitable for performance-critical applications on the web. It was a good learning experience over all.\nConclusion Overall, the Open Source Summit North America provided valuable insights, networking opportunities, and knowledge sharing, making it a remarkable event for open source enthusiasts and professionals alike.\n","permalink":"https://journalctl.org/post/ossna2023/","summary":"From May 10th to 12th, the Vancouver Convention Center came alive with the Open Source Summit North America, a three-day event that brought together open source software enthusiasts from around the world. With a focus on exploring the newest trends and embracing opportunities in the ever-changing field of open source technology, this premier conference drew over two thousand attendees. In this blog post, we\u0026rsquo;ll dive into my own presentation and its key points, share some highlights from the inspiring talks I attended, and take a closer look at the exciting conversations and new discoveries that unfolded at the Red Hat booth.","title":"Diving into the Open Source Ocean: A Recap of the Summit's Key Moments"},{"content":"Bio I am a skilled professional with a strong background in Linux system administration, and software engineering. I hold an MSc. in Computer Science from Queen’s University, where my research focused on software supply chain security. With expertise in DevOps tools such as Ansible, Docker, Kubernetes, OpenShift, Puppet, Terraform, and Vagrant, I excel in automating software systems and infrastructure. I consistently deliver high-quality code, implement critical features, and actively contribute to open-source projects. My passion lies in leveraging technology to enhance supply chain security and ensure the reliability and integrity of software systems.\nFind me on your favorite Social Media Handle Github rabajaj0509 Email rahulrb0509@gmail.com LinkedIn rahulbajaj0509 ","permalink":"https://journalctl.org/about/","summary":"Bio I am a skilled professional with a strong background in Linux system administration, and software engineering. I hold an MSc. in Computer Science from Queen’s University, where my research focused on software supply chain security. With expertise in DevOps tools such as Ansible, Docker, Kubernetes, OpenShift, Puppet, Terraform, and Vagrant, I excel in automating software systems and infrastructure. I consistently deliver high-quality code, implement critical features, and actively contribute to open-source projects.","title":"About me"}]