[{
    "title": "Diving into the Open Source Ocean: A Recap of the Summit's Key Moments",
    "date": "",
    "description": "",
    "body": "From May 10th to 12th, the Vancouver Convention Center came alive with the Open Source Summit North America, a three-day event that brought together open source software enthusiasts from around the world. With a focus on exploring the newest trends and embracing opportunities in the ever-changing field of open source technology, this premier conference drew over two thousand attendees. In this blog post, we\u0026rsquo;ll dive into my own presentation and its key points, share some highlights from the inspiring talks I attended, and take a closer look at the exciting conversations and new discoveries that unfolded at the Red Hat booth.\nDemystifying Unreproducible Builds: What, Why, and How? Soon after the keynote, the conference dispersed into multiple tracks. I presented in the Supply Chain Security track, where I discussed the Reproducible Builds initiative. My presentation covered several key topics:\n Exploring software supply chains and the challenges of ensuring supply chain security, despite their benefits. Understanding reproducible builds and how they mitigate the risk of software supply chain attacks on packages within a Linux distribution. Identifying the software that should be prioritized first for reproducibility within a Linux distribution. Examining the time it takes for developers to address unreproducible builds and identifying potential causes. Investigating the influence of external ecosystem factors on reproducibility in package builds.  By addressing these topics, I aimed to shed light on the importance of reproducible builds for enhancing supply chain security and ensuring the integrity of software packages within the Linux ecosystem.\nDuring my talk, I had the pleasure of meeting Mattia Rizzolo, a dedicated core maintainer of the Reproducible Builds project who had traveled all the way from Italy. It was fascinating to connect with someone so deeply involved in the subject matter. We discussed my research paper, which explored reproducible builds and was recently submitted to the Journal of Empirical Software Engineering. Mattia expressed genuine interest in reviewing it and even offered his assistance in addressing any potential rebuttals that may arise during the review process.\nMoreover, I was thrilled when Mattia appreciated the taxonomy of issues we had developed in the paper, which aimed to identify the causes of unreproducible builds. He went a step further and suggested that I open a pull request on the Reproducible Builds website to showcase this taxonomy. It was truly gratifying to receive recognition from a respected member of the project\u0026rsquo;s core team.\nAs our conversation continued, I couldn\u0026rsquo;t resist asking Mattia about his thoughts on the future of the Reproducible Builds project. He acknowledged that at present, many projects seem more inclined towards adopting trendy technologies and focusing on new developments, such as Software Bill of Materials (SBOM) files to secure the software supply chain. However, he raised a critical question: Can we truly guarantee that the SBOM file generated at the project level will remain unchanged for the customers and end users who consume it?\nEngaging sessions that caught my attention Culture Change: Building an OSPO in a Proprietary Software Company - Ruth Suehle, SAS In her talk, Ruth highlights that organizations in the software industry widely use open source software but questions their active contribution to the community. She cautions against the practice of solely forking and internally maintaining open source projects, which harms the community.\nFurthermore, Ruth advocates for establishing an Open Source Program Office (OSPO) team in proprietary software companies to address how they can contribute back. This team aligns values and actions with open source principles, sets guidelines, ensures compliance, and fosters collaboration. Lastly, Ruth emphasizes that embracing open source values leads to meaningful contributions and innovation, despite the time required for cultural change.\nThings I wish I had learned earlier about containers - Alex Juarez, Red Hat During the conference, Alex delivered an informative session that delved into the fundamental aspects of containers, making it an ideal starting point for beginners in the field. He covered several essential topics, including container runtimes, namespaces, cgroups, and binding volumes in containers. Additionally, he discussed the process of inspecting container images to identify exposed ports. If you\u0026rsquo;re interested in exploring the details further, you can find Alex\u0026rsquo;s slides for the session here.\nImplementing the OpenSSF Best Practices Badges \u0026amp; Scorecards Into Your Project - CRob, Intel \u0026amp; David A. Wheeler, The Linux Foundation Projects like Scorecard, operating under the OpenSSF umbrella, offer a valuable means to assess whether a software repository on GitHub adheres to important rules that mitigate supply chain attacks. These checks include the presence of a license file, the implementation of a CI pipeline for the project, and the utilization of a dependency manager such as Dependabot to analyze any vulnerabilities that may arise in current dependencies.\nAfter the talk, I had a brief discussion with David Wheeler regarding Scorecard\u0026rsquo;s limitations in evaluating proprietary software. Some of the checks were found to be incompatible with local systems, unlike the deployed version that performs checks on multiple repositories hosted on GitHub.\nMeeting David Wheeler in person was a source of great excitement for me. His blog post in 2021 about reproducible builds served as a motivating factor for my research. In his blog, he highlighted the importance of prioritizing certain software for reproducibility over others. This passage stimulated my interest in exploring the types of software that software developers should focus on in terms of reproducibility.\nInteresting conversations at the Red Hat Booth During my time at the Red Hat booth, I had the opportunity to engage in thought-provoking discussions with industry professionals.\nOne such encounter was with Jerry Cooperstein, an ex-Red Hatter who played a significant role in designing learning content for Red Hat\u0026rsquo;s courses and certifications. Currently, he leads efforts in designing courses and certifications for the Linux Foundation. Here are a few intriguing topics we covered:\n Jerry expressed his skepticism towards multiple-choice question (MCQ) based certifications that lack practical application and fail to assess the practical aspects of technology. He emphasized the importance of incorporating practical evaluations into certification programs. We delved into a discussion about the effectiveness of different learning mediums. Jerry firmly believed that books and written content leave a more profound impact on learners compared to video courses. According to him, entertainment should be separate from the learning process. On a lighter note, Jerry shared an interesting anecdote about the placement of the control key on keyboards. He mentioned that, in the past, the control key was located where the Caps Lock key is positioned today. However, due to the strain it caused on the pinky finger, the key\u0026rsquo;s placement was eventually changed.  During my conversation with the security software engineer at the Tenable booth, we delved deeper into the realm of container security. Our discussion covered various important topics, including the commonly performed security checks and best practices in the industry.\nWe explored the significance of following CIS benchmark guidelines, utilizing tools like Trivy or Clair to detect vulnerabilities, using muti-build container files, sanboxing containers (gVisor), and ensuring the use of security contexts for pods. These practices form the foundation for maintaining a secure container ecosystem.\nIntriguingly, our conversation expanded to address what differentiates companies like Tenable in this highly competitive industry. While acknowledging the similarity in security practices, the engineer highlighted a few additional insights that set companies apart. Some of these included:\n Zero-day vulnerability: We discussed the importance of actively monitoring and addressing zero-day vulnerabilities, which are unknown and unpatched vulnerabilities that can be exploited by attackers. Lateral movement: This topic focused on understanding and mitigating the techniques used by attackers to move laterally within a network or system after gaining initial access. By identifying and preventing lateral movement, organizations can limit the impact of potential security breaches. Active checking for vulnerabilities: We explored the proactive approach of actively scanning and checking for vulnerabilities in containerized applications. By actively assessing and addressing vulnerabilities, organizations can reduce the risk of exploitation. Exclusion warriors: This term refers to individuals or teams responsible for managing exclusion lists or rules in security systems. Their role is to specify what should be excluded from security scans or monitoring, ensuring efficient and accurate security operations.  I had seen the dockers adoption of wasm in Kubecon last year, but did pay much attention to the same. During a conversation at the Fermyon both, I was introduced to webassembly. I discovered for myself that WebAssembly (often abbreviated as Wasm) is a binary instruction format and virtual machine that enables the execution of high-performance, low-level code on the web. It is a portable, safe, and efficient technology designed to run in web browsers alongside JavaScript and other web languages.\nWebAssembly allows developers to compile code from various programming languages, such as C, C++, Rust, and more, into a compact binary format that can be executed by the web browser. This format is designed to be fast to load and execute, making it suitable for performance-critical applications on the web. It was a good learning experience over all.\nConclusion Overall, the Open Source Summit North America provided valuable insights, networking opportunities, and knowledge sharing, making it a remarkable event for open source enthusiasts and professionals alike.\n",
    "ref": "/post/ossna2023/"
  },{
    "title": "Managing Containers with Systemd",
    "date": "",
    "description": "To reduce the need for manual intervention, use systemd to manage container-based applications on a health check.",
    "body": "Containers are inherently ephemeral, making them more difficult to manage than traditional programmes operating on virtual or bare metal servers. Container monitoring, on the other hand, is a critical capability for applications based on current microservices architectures in order to achieve maximum performance.\nContainerized applications frequently necessitate monitoring. Performing health-checks is one technique to keep these containers up and running at all times. The usage of the curl command to verify if the application within the container is still up and running is one of the approaches I\u0026rsquo;ve come across for doing health-checks. The cronjob utility is used to perform curl commands on the container application at a periodic basis. If the container is not in a running state, a configuration management tool like Ansible can be used to start/restart it. Although this method does not need direct human intervention, it does not seem to have a mechanism for logging the event i.e. reason for downtime of the application/container. Therefore, it is not the best option for enhancing the container\u0026rsquo;s self-healing capabilities.\nInstead, we make use of the systemd utility of Linux which is ubiquitous across most of the Linux distributions. The systemd utility makes it easy to dynamically start/restart/stop configuration files for each deamon.\nIn this blog, we will:\n Create an Ansible role. Define the required variables. Create the service file usign a Jinja2 template Manage unit files within the containers using a playbook  With the aid of systemd, we\u0026rsquo;ll start an apache container and control its daemon. The Jinja2 templates are used to construct the unit file. We save the template files in the /usr/lib/systemd/system/ directory after they\u0026rsquo;ve been created. We can then use thesystemd ansible module to administer the deamon once the files have been stored in the correct location with the required permissions.\nCreate an Ansible role cd roles mkdir -p apache/{defaults,tasks,templates,handlers} Define the required variables --- apache_image: \u0026#34;httpd\u0026#34; apache_image_tag: \u0026#34;latest\u0026#34; Create the service file usign a Jinja2 template [Unit] Description=run apache container After=network.target docker.service Requires=docker.service [Service] ExecStartPre=-/usr/bin/docker stop apache ExecStartPre=-/usr/bin/docker rm apache ExecStartPre=/usr/bin/docker pull {{ apache_image }}:{{ apache_image_tag }} ExecStart=/usr/bin/docker run \\ --port=\u0026#39;8080:8080\u0026#39; \\ --name=apache \\ --volume={{ apache_data_dir }}:{{ apache_data_container_dir }}:ro \\ --restart=always \\ --log-driver=journald \\ {{ apache_image }}:{{ apache_image_tag }} Restart=always [Install] WantedBy=multi-user.target Manage unit files within the containers using a playbook /roles/apache/handlers/main.yml --- - name: restart container-apache systemd: name: container-apache.service daemon_reload: yes state: restarted /roles/apache/tasks/main.yml --- - name: set selinux boolean value seboolean: name: container_manage_cgroup state: yes persistent: yes - name: create the apache container unit file template: src: container-apache.service.j2 dest: /usr/lib/systemd/system/container-apache.service owner: root mode: 0660 notify: restart container-apache - name: link and enable container-apacahe systemd: name: container-apache enabled: yes - name: run apache container systemd: name: container-apache state: started ",
    "ref": "/post/systemd-containers/"
  },{
    "title": "About me",
    "date": "",
    "description": "",
    "body": " I am a highly skilled professional with a strong background in DevOps, Linux system administration, and software engineering. I hold a MSc. in Computer Science from the Queen\u0026rsquo;s University, where my research focused on software supply chain security. With expertise in DevOps tools such as Ansible, Docker, Kubernetes, Puppet, Terraform, and Vagrant, I excel in automating software systems and infrastructure. I consistently deliver high-quality code, implemented critical features, and actively contribute to open-source projects. My passion lies in leveraging technology to enhance supply chain security and ensure the reliability and integrity of software systems.  Contact  Github: rabajaj0509 Twitter: @rabajaj_ Email: rahulrb0509@gmail.com \u0026amp; rabajaj@redhat.com  ",
    "ref": "/about/"
  }]
