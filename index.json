[{
    "title": "Enhancing Security: Using Renovate in GitLab Pipelines for Automated Dependency Updates",
    "date": "",
    "description": "",
    "body": "Open Source Software (OSS) projects have been distributed in packages for decades. Using packages allows developers to focus on new feature implementation. Major software distributions, such as Fedora, Debian, etc, typically consist of thousands of packages. These packages depend on each other to perform tasks efficiently by avoiding code duplication. The inter-dependence amongst the packages creates a software supply chain.\nExamples of software security principles such as using software binaries that are signed by the software vendor, keeping binaries regularly updated, and constantly monitoring software behaviour are general to any software system. However, following these principles may not suffice for preventing a software supply chain from being infected with malicious code. Software supply chains are far from being simple. Recent supply chain attacks such as SolarWinds and Mimecast have raised serious security concerns. Despite the best effort of developers in addressing security concerns, any software project is prone to supply chain attacks. This is observed even in the context of major software organizations.\nSecuring your software supply chain goes beyond basic principals, it demands a proactive and adaptive appoach, especially when sophisticated threats are seen in the recent attacks. Renovate is becoming a crucial tool in this effort. Renovate, a powerful dependency management tool, steps in to enhance your defense strategy. Unlike traditional update approaches, Renovate actively monitors and patches vulnerabilities, making it a key player in safeguarding your software supply chain. In this blog, we\u0026rsquo;ll explore how integrating Renovate into GitLab CI pipelines can elevate your security measures, ensuring your project stays resilient against evolving cyber threats.\nImplementation Approach There are multiple ways to configure the GitLab Runner, which is responsible for running CI/CD pipelines on GitLab. In the context of this implementation, each CI/CD pipeline runs in a separate container within the OpenShift instance. To facilitate this, a Dockerfile is used to generate the container image that will be executed through OpenShift. In this implementation, a container image is generated for Renovate, and this container image is executed as a cron job resource within the OpenShift cluster.\nDockerfile Few key points to take note from the Dockerfile provided below are:\n We need to install the go and java programming languages in the Dockerfile to be able to update the go.sum and pom.xml files. Renovate works by inspecting configuration files in the application projects to identify the dependencies and their versions. Since we install the go programming language from the source, we verify its checksum to avoid any discrepancies and thereby refrain from installing malicious code We run the Renovate command as a non-root user.  FROMnode:latestUSERrootRUN mkdir -p /etc/rhsm/ca \u0026amp;\u0026amp; \\  rm -f /etc/rhsm-hostRUN npm install --global renovate yarn \u0026amp;\u0026amp; \\  curl -sfL --retry 10 -o /tmp/golang.tar.gz https://go.dev/dl/go1.21.3.linux-amd64.tar.gz \u0026amp;\u0026amp; \\  echo \u0026#34;1241381b2843fae5a9707eec1f8fb2ef94d827990582c7c7c32f5bdfbfd420c8 /tmp/golang.tar.gz\u0026#34; | sha256sum -c \u0026amp;\u0026amp; \\  mkdir -p /usr/local/go \u0026amp;\u0026amp; \\  tar --strip-components=1 -zxf /tmp/golang.tar.gz --directory /usr/local/go \u0026amp;\u0026amp; \\  mkdir -p /go/{path,cache,xdg} \u0026amp;\u0026amp; \\  chmod -R 777 /go \u0026amp;\u0026amp; \\  chown -R default /opt/app-root/srcRUN dnf install -y --disablerepo=* --enablerepo=ubi-9-baseos-rpms --enablerepo=ubi-9-appstream-rpms java-17-openjdk-develUSER1001ENV GOPATH /go/pathENV GOCACHE /go/cacheENV PATH $PATH:/usr/local/go/binCMD [\u0026#34;renovate\u0026#34;]",
    "ref": "/post/gitlab-renovate/"
  },{
    "title": "Slimming Down Containers: The Art of Minimizing Image Bloat",
    "date": "",
    "description": "",
    "body": "OpenShift, an enterprise-ready Kubernetes platform, offers a multitude of benefits. One such advantage is the Source-to-Image (S2I) build strategy, that simplifies the process of converting source code into deployable container images. This strategy enables developers to build container images without the need to define a container file explicitly. OpenShift clones the application\u0026rsquo;s source code into a builder image that utilizes builder scripts, ultimately generating a container image deployable within the cluster.\nEmploying the S2I strategy proves convenient, allowing software development teams to prioritize software development over the creation of container files. However, a notable drawback emerges as this approach tends to generate oversized images. These bulky images can rapidly consume storage capacity within private registries, resulting in prolonged push and pull times from the image registry. Beyond storage concerns, larger image sizes impact the build process, slowing down the overall build time. Moreover, heightened image size correlates with increased security risks, creating a larger attack surface for potential supply chain attacks due to the inclusion of numerous packages. Recognizing these challenges, the focus turns to exploring strategies aimed at slimming down containers.\nOur team adopted four primary practices to overcome this issue:\n Using small or minimal base images. Avoiding multiple RUN instructions. Excluding files and directories from the build context. Utilizing multi-stage container builds.  Using small or minimal base image Using small or minimal base images for containers involves selecting lightweight starting points that contain only the essential packages required to run your application. Opting for such base images, like Alpine or distroless images, significantly reduces the container size. It is crucial to identify base images that tailor to specific application needs, ensuring they contain the necessary libraries and dependencies while excluding unnecessary components. Minimizing the attack surface by selecting base images with fewer pre-installed packages also enhances container security.\nAvoiding multiple RUN instructions Avoiding multiple RUN instructions in Dockerfiles involves consolidating commands into a single RUN instruction wherever possible. By chaining commands together, we can reduce the number of intermediate layers created during the image build process. For instance, rather than having separate RUN commands for installing dependencies, copying files, and performing cleanup, we can combine these steps within a single RUN instruction. Here\u0026rsquo;s an example:\n# Bad practice: Multiple RUN instructionsFROMbaseimage:latestRUN dnf updateRUN dnf install -y package1RUN dnf install -y package2RUN dnf clean# Better practice: Consolidating commands in a single RUN instructionFROMbaseimage:latestRUN dnf update \\  \u0026amp;\u0026amp; dnf install -y package1 package2 \\  \u0026amp;\u0026amp; dnf cleanExcluding files and directories from the build context We can make use of the .containerignore or .dockerignore file in the build context directory to prevent unnecessary files and directories from being copied in to the image.\nAn example of .dockerignore file tailored for Ruby application:\n# Ignore version control files .git .gitignore # Ignore unnecessary editor/IDE files .vscode/ .idea/ # Ignore log and temp files log/ tmp/ # Ignore dependency manager specific files Gemfile.lock # Ignore development and test-specific files spec/ test/ Utilizing multi-stage container builds Multistage builds are implemented by defining multiple \u0026lsquo;stages\u0026rsquo; within a single Dockerfile. This strategy enables the separation of the build environment from the final runtime environment, leading to the creation of smaller and more secure container images. With multistage builds, each \u0026lsquo;stage\u0026rsquo; performs specific tasks, ensuring that the final container image only includes the necessary artifacts generated from the preceding build \u0026lsquo;stage.\u0026rsquo; This optimization results in a streamlined image tailored precisely to run the application, eliminating unnecessary packages or artifacts.\nAn example of the multi-stage build can be seen below:\n# Stage 1: Build stageFROMopenjdk-17 AS builderWORKDIR/appCOPY . .USERrootRUN apt-get update \u0026amp;\u0026amp; apt-get install -y some-dependencyRUN mvn -Dmaven.repo.local=/tmp/.m2/repository package# Stage 2: Runtime stageFROMopenjdk17:alpine-jreWORKDIR/appCOPY --from=builder --chown=someuser:somegroup /app/target/myapp.jar .USERsomeuserCMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;myapp.jar\u0026#34;]The example above illustrates a two-stage process within a container file. The first stage handles all build operations, while the second stage exclusively copies the artifact generated in the first stage. This artifact is used for deploying the image to the cluster. Two key points emerge from this example:\n Using different base image in the build and runtime stages. During the build stage, more packages may be necessary. For instance, in a Java application, we employ OpenJDK, the Java Development Kit (JDK), crucial for development and building. Conversely, the runtime stage utilizes the Java Runtime Environment (JRE), a subset of the JDK containing essential libraries to run the Java application. Hence, choosing base images tailored to specific needs is essential. In the build stage, we employ USER root to execute all build-related tasks. This approach often becomes necessary for tasks like installing dependencies and setting up the environment, requiring elevated privileges. However, once the necessary artifacts are acquired, transitioning to a non-root user USER someuser in subsequent stages enhances security. It minimizes potential vulnerabilities and restricts access privileges within the final runtime environment, improving overall security posture.  Conclusion: Lean container images are crucial for streamlined deployments, reducing storage overhead and minimizing resource consumption.\n",
    "ref": "/post/lean-container-images/"
  },{
    "title": "Managing Containers with Systemd",
    "date": "",
    "description": "To reduce the need for manual intervention, use systemd to manage container-based applications on a health check.",
    "body": "Containers are inherently ephemeral, making them more difficult to manage than traditional programmes operating on virtual or bare metal servers. Container monitoring, on the other hand, is a critical capability for applications based on current microservices architectures in order to achieve maximum performance.\nContainerized applications frequently necessitate monitoring. Performing health-checks is one technique to keep these containers up and running at all times. The usage of the curl command to verify if the application within the container is still up and running is one of the approaches I\u0026rsquo;ve come across for doing health-checks. The cronjob utility is used to perform curl commands on the container application at a periodic basis. If the container is not in a running state, a configuration management tool like Ansible can be used to start/restart it. Although this method does not need direct human intervention, it does not seem to have a mechanism for logging the event i.e. reason for downtime of the application/container. Therefore, it is not the best option for enhancing the container\u0026rsquo;s self-healing capabilities.\nInstead, we make use of the systemd utility of Linux which is ubiquitous across most of the Linux distributions. The systemd utility makes it easy to dynamically start/restart/stop configuration files for each deamon.\nIn this blog, we will:\n Create an Ansible role. Define the required variables. Create the service file usign a Jinja2 template Manage unit files within the containers using a playbook  With the aid of systemd, we\u0026rsquo;ll start an apache container and control its daemon. The Jinja2 templates are used to construct the unit file. We save the template files in the /usr/lib/systemd/system/ directory after they\u0026rsquo;ve been created. We can then use thesystemd ansible module to administer the deamon once the files have been stored in the correct location with the required permissions.\nCreate an Ansible role cd roles mkdir -p apache/{defaults,tasks,templates,handlers} Define the required variables --- apache_image: \u0026#34;httpd\u0026#34; apache_image_tag: \u0026#34;latest\u0026#34; Create the service file usign a Jinja2 template [Unit] Description=run apache container After=network.target docker.service Requires=docker.service [Service] ExecStartPre=-/usr/bin/docker stop apache ExecStartPre=-/usr/bin/docker rm apache ExecStartPre=/usr/bin/docker pull {{ apache_image }}:{{ apache_image_tag }} ExecStart=/usr/bin/docker run \\ --port=\u0026#39;8080:8080\u0026#39; \\ --name=apache \\ --volume={{ apache_data_dir }}:{{ apache_data_container_dir }}:ro \\ --restart=always \\ --log-driver=journald \\ {{ apache_image }}:{{ apache_image_tag }} Restart=always [Install] WantedBy=multi-user.target Let\u0026rsquo;s break down the important parts:\n Unit Section:   Description: Describes the purpose of the service. After: Defines the dependencies that must be started before this service. Requires: Specifies the units that this unit depends on.  Service Section:    ExecStartPre: Commands to be executed before starting the service. Here, it stops and removes any existing Docker container named \u0026lsquo;apache\u0026rsquo;. Then it pulls the specified Docker image (using variables apache_image and apache_image_tag provided during template rendering).\n  ExecStart: Command to start the Docker container with various options:\n\u0026ndash;port=\u0026lsquo;8080:8080\u0026rsquo;: Maps port 8080 from the container to the host.\n\u0026ndash;name=apache: Assigns the name \u0026lsquo;apache\u0026rsquo; to the container.\n\u0026ndash;volume={{ apache_data_dir }}:{{ apache_data_container_dir }}:ro: Mounts a directory from the host to a directory in the container in read-only mode.\n\u0026ndash;restart=always: Specifies that the container should always restart if it stops.\n\u0026ndash;log-driver=journald: Defines the logging driver for the container.\n{{ apache_image }}:{{ apache_image_tag }}: Specifies the Docker image and its tag to be used.\n  Restart: Defines the restart policy for the service, set to \u0026lsquo;always\u0026rsquo;.\n  Install Section:   WantedBy: Specifies the target that this service should be enabled for.  Manage unit files within the containers using a playbook /roles/apache/handlers/main.yml --- - name: restart container-apache systemd: name: container-apache.service daemon_reload: yes state: restarted /roles/apache/tasks/main.yml --- - name: set selinux boolean value seboolean: name: container_manage_cgroup state: yes persistent: yes - name: create the apache container unit file template: src: container-apache.service.j2 dest: /usr/lib/systemd/system/container-apache.service owner: root mode: 0660 notify: restart container-apache - name: link and enable container-apacahe systemd: name: container-apache enabled: yes - name: run apache container systemd: name: container-apache state: started ",
    "ref": "/post/systemd-containers/"
  },{
    "title": "Diving into the Open Source Ocean: A Recap of the Summit's Key Moments",
    "date": "",
    "description": "",
    "body": "From May 10th to 12th, the Vancouver Convention Center came alive with the Open Source Summit North America, a three-day event that brought together open source software enthusiasts from around the world. With a focus on exploring the newest trends and embracing opportunities in the ever-changing field of open source technology, this premier conference drew over two thousand attendees. In this blog post, we\u0026rsquo;ll dive into my own presentation and its key points, share some highlights from the inspiring talks I attended, and take a closer look at the exciting conversations and new discoveries that unfolded at the Red Hat booth.\nDemystifying Unreproducible Builds: What, Why, and How? Soon after the keynote, the conference dispersed into multiple tracks. I presented in the Supply Chain Security track, where I discussed the Reproducible Builds initiative. My presentation covered several key topics:\n Exploring software supply chains and the challenges of ensuring supply chain security, despite their benefits. Understanding reproducible builds and how they mitigate the risk of software supply chain attacks on packages within a Linux distribution. Identifying the software that should be prioritized first for reproducibility within a Linux distribution. Examining the time it takes for developers to address unreproducible builds and identifying potential causes. Investigating the influence of external ecosystem factors on reproducibility in package builds.  By addressing these topics, I aimed to shed light on the importance of reproducible builds for enhancing supply chain security and ensuring the integrity of software packages within the Linux ecosystem.\nDuring my talk, I had the pleasure of meeting Mattia Rizzolo, a dedicated core maintainer of the Reproducible Builds project who had traveled all the way from Italy. It was fascinating to connect with someone so deeply involved in the subject matter. We discussed my research paper, which explored reproducible builds and was recently submitted to the Journal of Empirical Software Engineering. Mattia expressed genuine interest in reviewing it and even offered his assistance in addressing any potential rebuttals that may arise during the review process.\nMoreover, I was thrilled when Mattia appreciated the taxonomy of issues we had developed in the paper, which aimed to identify the causes of unreproducible builds. He went a step further and suggested that I open a pull request on the Reproducible Builds website to showcase this taxonomy. It was truly gratifying to receive recognition from a respected member of the project\u0026rsquo;s core team.\nAs our conversation continued, I couldn\u0026rsquo;t resist asking Mattia about his thoughts on the future of the Reproducible Builds project. He acknowledged that at present, many projects seem more inclined towards adopting trendy technologies and focusing on new developments, such as Software Bill of Materials (SBOM) files to secure the software supply chain. However, he raised a critical question: Can we truly guarantee that the SBOM file generated at the project level will remain unchanged for the customers and end users who consume it?\nEngaging sessions that caught my attention Culture Change: Building an OSPO in a Proprietary Software Company - Ruth Suehle, SAS In her talk, Ruth highlights that organizations in the software industry widely use open source software but questions their active contribution to the community. She cautions against the practice of solely forking and internally maintaining open source projects, which harms the community.\nFurthermore, Ruth advocates for establishing an Open Source Program Office (OSPO) team in proprietary software companies to address how they can contribute back. This team aligns values and actions with open source principles, sets guidelines, ensures compliance, and fosters collaboration. Lastly, Ruth emphasizes that embracing open source values leads to meaningful contributions and innovation, despite the time required for cultural change.\nThings I wish I had learned earlier about containers - Alex Juarez, Red Hat During the conference, Alex delivered an informative session that delved into the fundamental aspects of containers, making it an ideal starting point for beginners in the field. He covered several essential topics, including container runtimes, namespaces, cgroups, and binding volumes in containers. Additionally, he discussed the process of inspecting container images to identify exposed ports. If you\u0026rsquo;re interested in exploring the details further, you can find Alex\u0026rsquo;s slides for the session here.\nImplementing the OpenSSF Best Practices Badges \u0026amp; Scorecards Into Your Project - CRob, Intel \u0026amp; David A. Wheeler, The Linux Foundation Projects like Scorecard, operating under the OpenSSF umbrella, offer a valuable means to assess whether a software repository on GitHub adheres to important rules that mitigate supply chain attacks. These checks include the presence of a license file, the implementation of a CI pipeline for the project, and the utilization of a dependency manager such as Dependabot to analyze any vulnerabilities that may arise in current dependencies.\nAfter the talk, I had a brief discussion with David Wheeler regarding Scorecard\u0026rsquo;s limitations in evaluating proprietary software. Some of the checks were found to be incompatible with local systems, unlike the deployed version that performs checks on multiple repositories hosted on GitHub.\nMeeting David Wheeler in person was a source of great excitement for me. His blog post in 2021 about reproducible builds served as a motivating factor for my research. In his blog, he highlighted the importance of prioritizing certain software for reproducibility over others. This passage stimulated my interest in exploring the types of software that software developers should focus on in terms of reproducibility.\nInteresting conversations at the Red Hat Booth During my time at the Red Hat booth, I had the opportunity to engage in thought-provoking discussions with industry professionals.\nOne such encounter was with Jerry Cooperstein, an ex-Red Hatter who played a significant role in designing learning content for Red Hat\u0026rsquo;s courses and certifications. Currently, he leads efforts in designing courses and certifications for the Linux Foundation. Here are a few intriguing topics we covered:\n Jerry expressed his skepticism towards multiple-choice question (MCQ) based certifications that lack practical application and fail to assess the practical aspects of technology. He emphasized the importance of incorporating practical evaluations into certification programs. We delved into a discussion about the effectiveness of different learning mediums. Jerry firmly believed that books and written content leave a more profound impact on learners compared to video courses. According to him, entertainment should be separate from the learning process. On a lighter note, Jerry shared an interesting anecdote about the placement of the control key on keyboards. He mentioned that, in the past, the control key was located where the Caps Lock key is positioned today. However, due to the strain it caused on the pinky finger, the key\u0026rsquo;s placement was eventually changed.  During my conversation with the security software engineer at the Tenable booth, we delved deeper into the realm of container security. Our discussion covered various important topics, including the commonly performed security checks and best practices in the industry.\nWe explored the significance of following CIS benchmark guidelines, utilizing tools like Trivy or Clair to detect vulnerabilities, using muti-build container files, sanboxing containers (gVisor), and ensuring the use of security contexts for pods. These practices form the foundation for maintaining a secure container ecosystem.\nIntriguingly, our conversation expanded to address what differentiates companies like Tenable in this highly competitive industry. While acknowledging the similarity in security practices, the engineer highlighted a few additional insights that set companies apart. Some of these included:\n Zero-day vulnerability: We discussed the importance of actively monitoring and addressing zero-day vulnerabilities, which are unknown and unpatched vulnerabilities that can be exploited by attackers. Lateral movement: This topic focused on understanding and mitigating the techniques used by attackers to move laterally within a network or system after gaining initial access. By identifying and preventing lateral movement, organizations can limit the impact of potential security breaches. Active checking for vulnerabilities: We explored the proactive approach of actively scanning and checking for vulnerabilities in containerized applications. By actively assessing and addressing vulnerabilities, organizations can reduce the risk of exploitation. Exclusion warriors: This term refers to individuals or teams responsible for managing exclusion lists or rules in security systems. Their role is to specify what should be excluded from security scans or monitoring, ensuring efficient and accurate security operations.  I had seen the dockers adoption of wasm in Kubecon last year, but did pay much attention to the same. During a conversation at the Fermyon both, I was introduced to webassembly. I discovered for myself that WebAssembly (often abbreviated as Wasm) is a binary instruction format and virtual machine that enables the execution of high-performance, low-level code on the web. It is a portable, safe, and efficient technology designed to run in web browsers alongside JavaScript and other web languages.\nWebAssembly allows developers to compile code from various programming languages, such as C, C++, Rust, and more, into a compact binary format that can be executed by the web browser. This format is designed to be fast to load and execute, making it suitable for performance-critical applications on the web. It was a good learning experience over all.\nConclusion Overall, the Open Source Summit North America provided valuable insights, networking opportunities, and knowledge sharing, making it a remarkable event for open source enthusiasts and professionals alike.\n",
    "ref": "/post/ossna2023/"
  },{
    "title": "About me",
    "date": "",
    "description": "",
    "body": " I am a skilled professional with a strong background in DevOps, Linux system administration, and software engineering. I hold a MSc. in Computer Science from the Queen\u0026rsquo;s University, where my research focused on software supply chain security. With expertise in DevOps tools such as Ansible, Docker, Kubernetes, Puppet, Terraform, and Vagrant, I excel in automating software systems and infrastructure. I consistently deliver high-quality code, implemented critical features, and actively contribute to open-source projects. My passion lies in leveraging technology to enhance supply chain security and ensure the reliability and integrity of software systems.  Contact  Github: rabajaj0509 Twitter: @rabajaj_ Email: rahulrb0509@gmail.com \u0026amp; rabajaj@redhat.com  ",
    "ref": "/about/"
  }]
